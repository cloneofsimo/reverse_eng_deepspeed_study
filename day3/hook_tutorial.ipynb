{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor hook\n",
    "\n",
    "- [https://pytorch.org/docs/stable/notes/autograd.html](https://pytorch.org/docs/stable/notes/autograd.html)\n",
    "\n",
    "- [https://pytorch.org/docs/stable/generated/torch.autograd.graph.Node.register_hook.html](https://pytorch.org/docs/stable/generated/torch.autograd.graph.Node.register_hook.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 2.2.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "print(f'torch.__version__: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# vanilla backprob\n",
    "v = torch.tensor([0., 0., 0.], requires_grad=True)\n",
    "print(v.grad)\n",
    "v.backward(torch.tensor([1., 2., 3.]))\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "# register hook to double the gradient\n",
    "v.grad = None\n",
    "h = v.register_hook(lambda x: x * 2)\n",
    "v.backward(torch.tensor([1., 2., 3.]))\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# removes the hook\n",
    "h.remove()\n",
    "v.grad = None\n",
    "v.backward(torch.tensor([1., 2., 3.]))\n",
    "print(v.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.)\n",
      "tensor(10.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True) # leaf\n",
    "b = torch.tensor(2.0, requires_grad=True) # leaf\n",
    "c = a*b # non-leaf\n",
    "\n",
    "def c_hook(grad):\n",
    "    print(grad)\n",
    "    return grad + 2\n",
    "\n",
    "c.retain_grad()\n",
    "c.register_hook(c_hook)\n",
    "c.register_hook(lambda grad: print(grad))\n",
    "\n",
    "d = torch.tensor(4.0, requires_grad=True) # leaf\n",
    "d.register_hook(lambda grad: grad + 100)\n",
    "\n",
    "e = c * d # noon-leaf\n",
    "e.retain_grad()\n",
    "e.register_hook(lambda grad: grad * 2)\n",
    "e.retain_grad()\n",
    "\n",
    "e.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([0., 0., 0.], requires_grad=True), True, None\n",
      "b: tensor([0., 0., 0.], grad_fn=<CloneBackward0>), True, <CloneBackward0 object at 0x7f6d9c337dc0>\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0., 0., 0.], requires_grad=True) # leaf node\n",
    "b = a.clone() # non-leaf node\n",
    "print(f'a: {a}, {b.requires_grad}, {a.grad_fn}')\n",
    "print(f'b: {b}, {b.requires_grad}, {b.grad_fn}')\n",
    "assert isinstance(b.grad_fn, torch.autograd.graph.Node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2.])\n",
      "None\n",
      "tensor([1., 1., 1.])\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79360/3718958541.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(b.grad)\n",
      "/tmp/ipykernel_79360/3718958541.py:14: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(b.grad)\n"
     ]
    }
   ],
   "source": [
    "# it double the gradient for backpropagation but it does not change its own gradient\n",
    "handle = b.grad_fn.register_hook(lambda gI, gO: (gO[0] * 2,)) # should define gradient input and output\n",
    "\n",
    "# because loss = b1 + b2 + b3, each gradient is equally 1. \n",
    "# and because we double propagted gradient, it should be [2., 2., 2.] for a\n",
    "b.sum().backward(retain_graph=True)\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "\n",
    "handle.remove() # Removes the hook\n",
    "a.grad, b.grad = None, None\n",
    "b.sum().backward(retain_graph=True)\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([0., 0., 0.], requires_grad=True), None\n",
      "b: tensor([0., 0., 0.], grad_fn=<CloneBackward0>), <CloneBackward0 object at 0x7f6da4b0ba30>\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0., 0., 0.], requires_grad=True) # leaf node\n",
    "b = a.clone() # non-leaf node\n",
    "b.retain_grad()\n",
    "print(f'a: {a}, {b.requires_grad}, {a.grad_fn}')\n",
    "print(f'b: {b}, {b.requires_grad}, {b.grad_fn}')\n",
    "assert isinstance(b.grad_fn, torch.autograd.graph.Node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2.])\n",
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# if \n",
    "handle = b.grad_fn.register_hook(lambda gI, gO: (gO[0] * 2,))\n",
    "b.sum().backward(retain_graph=True)\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "\n",
    "handle.remove() # Removes the hook\n",
    "a.grad, b.grad = None, None\n",
    "b.sum().backward(retain_graph=True)\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## module hook\n",
    "\n",
    "- [https://pytorch.org/docs/stable/notes/autograd.html#backward-hooks-execution](https://pytorch.org/docs/stable/notes/autograd.html#backward-hooks-execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoodNet(\n",
      "  (emb): Embedding(5, 8)\n",
      "  (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (out): Linear(in_features=8, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from src.models import get_dummy_mlp_model\n",
    "embedding_dim = 5\n",
    "hidden_dim = 8\n",
    "model = get_dummy_mlp_model(embedding_dim, hidden_dim, torch.bfloat16).cuda().train()\n",
    "device = next(iter(model.parameters())).device\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_print_hook(module, input, output):\n",
    "    print(f'''\n",
    "    module        : {module}\n",
    "    input         : {input}\n",
    "    output        : {output}\n",
    "    ''')\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    module.register_forward_hook(forward_print_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_accs = []\n",
    "def wrapper(param):\n",
    "    param_tmp = param.expand_as(param)\n",
    "    grad_acc = param_tmp.grad_fn.next_functions[0][0]\n",
    "\n",
    "    def print_grad_size(*notneeded):\n",
    "        print(param.grad.size())\n",
    "    grad_acc.register_hook(print_grad_size)\n",
    "    grad_accs.append(grad_acc)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        wrapper(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randint(5, (1,5)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    module        : Embedding(5, 8)\n",
      "    input         : (tensor([[3, 1, 0, 1, 0]], device='cuda:0'),)\n",
      "    output        : tensor([[[ 1.6641,  1.0312, -0.6055, -0.0190,  0.7773,  0.3750, -0.4453,\n",
      "           0.3418],\n",
      "         [-0.1748,  1.5469,  0.2988,  0.9102, -0.2334, -0.7695,  1.6016,\n",
      "           0.1992],\n",
      "         [ 0.6055, -0.2539,  1.1875, -0.7109,  0.8477, -0.7305, -0.2832,\n",
      "          -1.1797],\n",
      "         [-0.1748,  1.5469,  0.2988,  0.9102, -0.2334, -0.7695,  1.6016,\n",
      "           0.1992],\n",
      "         [ 0.6055, -0.2539,  1.1875, -0.7109,  0.8477, -0.7305, -0.2832,\n",
      "          -1.1797]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "    \n",
      "\n",
      "    module        : Linear(in_features=8, out_features=8, bias=True)\n",
      "    input         : (tensor([[[1.6641, 1.0312, 0.0000, 0.0000, 0.7773, 0.3750, 0.0000, 0.3418],\n",
      "         [0.0000, 1.5469, 0.2988, 0.9102, 0.0000, 0.0000, 1.6016, 0.1992],\n",
      "         [0.6055, 0.0000, 1.1875, 0.0000, 0.8477, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 1.5469, 0.2988, 0.9102, 0.0000, 0.0000, 1.6016, 0.1992],\n",
      "         [0.6055, 0.0000, 1.1875, 0.0000, 0.8477, 0.0000, 0.0000, 0.0000]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ReluBackward0>),)\n",
      "    output        : tensor([[[ 2.0508e-01, -8.6426e-02, -7.5684e-02,  1.0391e+00, -3.6865e-02,\n",
      "          -5.0000e-01,  2.0508e-01, -2.2949e-01],\n",
      "         [-4.7656e-01, -2.1777e-01, -1.5918e-01,  1.5991e-02, -2.0020e-02,\n",
      "           6.4453e-01, -7.6953e-01,  1.8652e-01],\n",
      "         [ 2.8125e-01,  5.7617e-02, -5.3906e-01,  2.5195e-01, -2.2754e-01,\n",
      "          -4.3106e-04,  5.3125e-01,  3.0762e-02],\n",
      "         [-4.7656e-01, -2.1777e-01, -1.5918e-01,  1.5991e-02, -2.0020e-02,\n",
      "           6.4453e-01, -7.6953e-01,  1.8652e-01],\n",
      "         [ 2.8125e-01,  5.7617e-02, -5.3906e-01,  2.5195e-01, -2.2754e-01,\n",
      "          -4.3106e-04,  5.3125e-01,  3.0762e-02]]], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "    \n",
      "\n",
      "    module        : Linear(in_features=8, out_features=5, bias=True)\n",
      "    input         : (tensor([[[0.2051, 0.0000, 0.0000, 1.0391, 0.0000, 0.0000, 0.2051, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0160, 0.0000, 0.6445, 0.0000, 0.1865],\n",
      "         [0.2812, 0.0576, 0.0000, 0.2520, 0.0000, 0.0000, 0.5312, 0.0308],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0160, 0.0000, 0.6445, 0.0000, 0.1865],\n",
      "         [0.2812, 0.0576, 0.0000, 0.2520, 0.0000, 0.0000, 0.5312, 0.0308]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ReluBackward0>),)\n",
      "    output        : tensor([[[-0.6836,  0.3613,  0.2344, -0.2305,  0.4023],\n",
      "         [-0.3027,  0.2969,  0.2217, -0.0266,  0.0146],\n",
      "         [-0.5703,  0.0284,  0.0986, -0.2559,  0.2832],\n",
      "         [-0.3027,  0.2969,  0.2217, -0.0266,  0.0146],\n",
      "         [-0.5703,  0.0284,  0.0986, -0.2559,  0.2832]]], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "    \n",
      "\n",
      "    module        : GoodNet(\n",
      "  (emb): Embedding(5, 8)\n",
      "  (fc2): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (out): Linear(in_features=8, out_features=5, bias=True)\n",
      ")\n",
      "    input         : (tensor([[3, 1, 0, 1, 0]], device='cuda:0'),)\n",
      "    output        : tensor([[[-0.6836,  0.3613,  0.2344, -0.2305,  0.4023],\n",
      "         [-0.3027,  0.2969,  0.2217, -0.0266,  0.0146],\n",
      "         [-0.5703,  0.0284,  0.0986, -0.2559,  0.2832],\n",
      "         [-0.3027,  0.2969,  0.2217, -0.0266,  0.0146],\n",
      "         [-0.5703,  0.0284,  0.0986, -0.2559,  0.2832]]], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<ViewBackward0>)\n",
      "    \n",
      "===========================================================================\n",
      "torch.Size([5])\n",
      "torch.Size([5, 8])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 8])\n",
      "torch.Size([5, 8])\n"
     ]
    }
   ],
   "source": [
    "logit = model(input)[:, :-1].contiguous().view(-1, embedding_dim)\n",
    "target = input[:, 1:].contiguous().view(-1)\n",
    "print('====='*15)\n",
    "F.cross_entropy(logit, target).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "ds_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
